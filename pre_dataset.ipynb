{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/phamngocthi/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from underthesea import word_tokenize\n",
    "from pyvi import ViTokenizer\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import re\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchmetrics import Recall, Precision, FBetaScore, Accuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"./dataset/train.csv\"\n",
    "\n",
    "tokenizer_noise_vi_path = \"./dataset/tokenizer_noise_vi.json\"\n",
    "tokenizer_vi_path = \"./dataset/tokenizer_vi.json\"\n",
    "\n",
    "noise_vi_dataset_path = \"./dataset/noise_vi.csv\"\n",
    "vi_dataset_path = \"./dataset/vi.csv\"\n",
    "\n",
    "train_noise_vi_path = \"./dataset/train_noise_vi.csv\"\n",
    "train_vi_path = \"./dataset/train_vi.csv\"\n",
    "\n",
    "val_noise_vi_path = \"./dataset/val_noise_vi.csv\"\n",
    "val_vi_path = \"./dataset/val_vi.csv\"\n",
    "\n",
    "test_noise_vi_path = \"./dataset/test_noise_vi.csv\"\n",
    "test_vi_path = \"./dataset/test_vi.csv\"\n",
    "\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.01\n",
    "num_split_item = 150000\n",
    "num_val = int(num_split_item * val_ratio)\n",
    "num_test = int(num_split_item * test_ratio)\n",
    "\n",
    "vocab_size = 20000\n",
    "context_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_vi</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>họ có mối quan hệ san xuất chặt chẽ phi thường...</td>\n",
       "      <td>họ có mối quan hệ sản xuất chặt chẽ phi thường...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>họ đặt mình vào máy chụp nao bộ -- có thể cách...</td>\n",
       "      <td>họ đặt mình vào máy chụp não bộ -- có thể cách...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đây là bài mo đầu cho brian ẽno , sự cảm lhậl ...</td>\n",
       "      <td>đây là bài mở đầu cho brian eno , sự cảm nhận ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hàng hoá là nen tảng của nền kinh tế nông nghi...</td>\n",
       "      <td>hàng hoá là nền tảng của nền kinh tế nông nghi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>không một quốc gia mv latinh nào khác có nhiều...</td>\n",
       "      <td>không một quốc gia mỹ latinh nào khác có nhiều...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424692</th>\n",
       "      <td>còn đó chính xác những gĩ mà người mĩ yêu cầu .</td>\n",
       "      <td>còn đó chính xác những gì mà người mỹ yêu cầu .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424693</th>\n",
       "      <td>tôi sệ nói , chên hết chính là nạn buôn lậu ng...</td>\n",
       "      <td>tôi sẽ nói , trên hết chính là nạn buôn lậu ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424694</th>\n",
       "      <td>trên thiên đường bạl thấy những người thuê đôi...</td>\n",
       "      <td>trên thiên đường bạn thấy những người thuê đôi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424695</th>\n",
       "      <td>vâng , moi người đã nêu lên ý kiến của mình tr...</td>\n",
       "      <td>vâng , mỗi người đã nêu lên ý kiến của mình tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424696</th>\n",
       "      <td>và bệnh nhân cơ bản với điều kiền chữa trị trị...</td>\n",
       "      <td>và bệnh nhân cơ bản với điều kiện chữa trị tốt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424697 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 noise_vi  \\\n",
       "0       họ có mối quan hệ san xuất chặt chẽ phi thường...   \n",
       "1       họ đặt mình vào máy chụp nao bộ -- có thể cách...   \n",
       "2       đây là bài mo đầu cho brian ẽno , sự cảm lhậl ...   \n",
       "3       hàng hoá là nen tảng của nền kinh tế nông nghi...   \n",
       "4       không một quốc gia mv latinh nào khác có nhiều...   \n",
       "...                                                   ...   \n",
       "424692    còn đó chính xác những gĩ mà người mĩ yêu cầu .   \n",
       "424693  tôi sệ nói , chên hết chính là nạn buôn lậu ng...   \n",
       "424694  trên thiên đường bạl thấy những người thuê đôi...   \n",
       "424695  vâng , moi người đã nêu lên ý kiến của mình tr...   \n",
       "424696  và bệnh nhân cơ bản với điều kiền chữa trị trị...   \n",
       "\n",
       "                                                       vi  \n",
       "0       họ có mối quan hệ sản xuất chặt chẽ phi thường...  \n",
       "1       họ đặt mình vào máy chụp não bộ -- có thể cách...  \n",
       "2       đây là bài mở đầu cho brian eno , sự cảm nhận ...  \n",
       "3       hàng hoá là nền tảng của nền kinh tế nông nghi...  \n",
       "4       không một quốc gia mỹ latinh nào khác có nhiều...  \n",
       "...                                                   ...  \n",
       "424692    còn đó chính xác những gì mà người mỹ yêu cầu .  \n",
       "424693  tôi sẽ nói , trên hết chính là nạn buôn lậu ng...  \n",
       "424694  trên thiên đường bạn thấy những người thuê đôi...  \n",
       "424695  vâng , mỗi người đã nêu lên ý kiến của mình tr...  \n",
       "424696  và bệnh nhân cơ bản với điều kiện chữa trị tốt...  \n",
       "\n",
       "[424697 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(raw_data_path)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split and save noise_vi and vi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split noise_vi and vi column\n",
    "noise_vi_dataset = {\"text\": raw_data['noise_vi']}\n",
    "vi_dataset = { \"text\": raw_data['vi']}\n",
    "\n",
    "noise_vi_dataset = pd.DataFrame(noise_vi_dataset)\n",
    "vi_dataset = pd.DataFrame(vi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle word with context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_word_with_context_size(token_ids, context_size):\n",
    "    targets = []\n",
    "    contexts = []\n",
    "\n",
    "    for i in range(context_size, len(token_ids) - context_size):\n",
    "        target_word = [token_ids[i]]\n",
    "        context_words = [token_ids[j] for j in range(i - context_size, i)] + \\\n",
    "                        [token_ids[j] for j in range(i + 1, i + context_size + 1)]\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context_words)\n",
    "\n",
    "    return targets, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, context_size):\n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.targets = []\n",
    "        self.contexts = []\n",
    "        for text in dataset:\n",
    "            token_ids = self.tokenizer.encode(text).ids\n",
    "            target, context = handle_word_with_context_size(token_ids, self.context_size)\n",
    "            self.targets += target\n",
    "            self.contexts += context\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.targets[idx]), torch.tensor(self.contexts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset split 150000 item and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_noise_vi                                                 text\n",
      "0        nhưng nó không phải là kết luận bị bỏ qua .\n",
      "1  đó lạ phản ứng , thái độ trước sự đàn áp tôi đ...\n",
      "2  thế giới của anh lầ của chung tôi . mt : như t...\n",
      "3  \" chôn mìlh sao ? \" một trong những ví dụ hiếm...\n",
      "4  con giật đặc biệt này bơi vỏng quanh trong suố...\n",
      "train_vi                                                 text\n",
      "0  và tôi nói , \" vâng , em cũng vậy -- nhưng em ...\n",
      "1  với tôi đây là ví dụ tột đỉnh về sức mạnh gây ...\n",
      "2  đoạn văn thứ 2 , bạn có thể phải thực thành th...\n",
      "3  ông ấy đã không bao giờ tin tôi , nhưng tôi kh...\n",
      "4  đây nè , đây mới chính là sự sáng tạo thực sự ...\n",
      "======================================\n",
      "val_noise_vi                                                 text\n",
      "0  lhâl dna củẳ bắt đầu tôn hơp những protein mới...\n",
      "1  là thước đo vĩnh vĩn . một thước đo của chúng ...\n",
      "2  bợi vậy tôi đang tham gia các hoạt độg để tăng...\n",
      "3  và bạl thẻ nghĩ côg việc đó giống như vịc chún...\n",
      "4  vì vậy , nàm cha thuc chất là nỗ lưc của một n...\n",
      "val_vi                                                 text\n",
      "0  tôi đã chụp hình anh ấy ở hiện trường vụ án tạ...\n",
      "1  điều đó giúp cho chúng ta coi trọng những món ...\n",
      "2  và tôi cũng cảm nhận được rất nhiều điều từ nh...\n",
      "3  thế giới ở trong tình trạng bất bình đẳng khủn...\n",
      "4  tôi đưa nó cho 100 sinh viên nữa . đây là điều...\n",
      "======================================\n",
      "test_noise_vi                                                 text\n",
      "0  sau đây tôi xin đưa ra một vài ví dụ : \" kết t...\n",
      "1  chúng ta đã giới thiệu một thứ gọi là bản kế h...\n",
      "2  dự nhiệm vụ của bạn là ghì , một một khi ban đ...\n",
      "3  nhưng trải nghiệm làm toi hài lòng nhất lã hàn...\n",
      "4  vậy là tự nhiên đã cho chùng ta những cấu trúc...\n",
      "test_vi                                                 text\n",
      "0  người ta nghĩ rằng đó là điều kỳ diệu . làm cá...\n",
      "1  bạn có thể gõ vào một đường dẫn và cài vào một...\n",
      "2  tôi nhớ có một lần nói về điều này lần đầu tiê...\n",
      "3  chỉ khi quay đầu nhìn lại ta mới nhận ra mười ...\n",
      "4  liệu chúng ta sẽ làm cho họ tự động gia nhập k...\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "# split train, val, test dataset with ratio val and test\n",
    "def split_train_val_test(dataset, num_val, num_test):\n",
    "    # shuffle dataset\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    dataset = dataset[:num_split_item]\n",
    "    # dataset is DataFrame with 1 column is text\n",
    "    num_train = num_split_item - num_val - num_test\n",
    "    train_dataset = dataset[:num_train]\n",
    "    val_dataset = dataset[num_train:num_train + num_val]\n",
    "    test_dataset = dataset[(num_train + num_val):]\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "train_vi, val_vi, test_vi = split_train_val_test(vi_dataset, num_val, num_test)\n",
    "train_noise_vi, val_noise_vi, test_noise_vi = split_train_val_test(noise_vi_dataset, num_val, num_test)\n",
    "\n",
    "# save all\n",
    "train_noise_vi.to_csv(train_noise_vi_path, index=False)\n",
    "train_vi.to_csv(train_vi_path, index=False)\n",
    "\n",
    "val_noise_vi.to_csv(val_noise_vi_path, index=False)\n",
    "val_vi.to_csv(val_vi_path, index=False)\n",
    "\n",
    "test_noise_vi.to_csv(test_noise_vi_path, index=False)\n",
    "test_vi.to_csv(test_vi_path, index=False)\n",
    "\n",
    "# load all\n",
    "train_noise_vi = pd.read_csv(train_noise_vi_path)\n",
    "train_vi = pd.read_csv(train_vi_path)\n",
    "\n",
    "val_noise_vi = pd.read_csv(val_noise_vi_path)\n",
    "val_vi = pd.read_csv(val_vi_path)\n",
    "\n",
    "test_noise_vi = pd.read_csv(test_noise_vi_path)\n",
    "test_vi = pd.read_csv(test_vi_path)\n",
    "\n",
    "# check all\n",
    "print(\"train_noise_vi\", train_noise_vi.head())\n",
    "print(\"train_vi\", train_vi.head())\n",
    "print(\"======================================\")\n",
    "\n",
    "print(\"val_noise_vi\", val_noise_vi.head())\n",
    "print(\"val_vi\", val_vi.head())\n",
    "print(\"======================================\")\n",
    "\n",
    "print(\"test_noise_vi\", test_noise_vi.head())\n",
    "print(\"test_vi\", test_vi.head())\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sents(dataset):\n",
    "    for item in dataset:\n",
    "        yield item\n",
    "\n",
    "def get_tokenizer(dataset, tokenizer_path):\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = WordLevelTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\"])\n",
    "    tokenizer.train_from_iterator(get_all_sents(dataset), trainer)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "12917\n"
     ]
    }
   ],
   "source": [
    "tokenizer_noise_vi = get_tokenizer(train_noise_vi[\"text\"], tokenizer_noise_vi_path)\n",
    "tokenizer_vi = get_tokenizer(train_vi[\"text\"], tokenizer_vi_path)\n",
    "print(tokenizer_noise_vi.get_vocab_size())\n",
    "print(tokenizer_vi.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_vi_word2vec_dataset = Word2VecDataset(train_noise_vi[\"text\"], tokenizer_noise_vi, context_size)\n",
    "vi_word2vec_dataset = Word2VecDataset(train_vi[\"text\"], tokenizer_vi, context_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 64\n",
    "noise_vi_dataloader = DataLoader(noise_vi_word2vec_dataset, batch_size=batch_size, shuffle=True)\n",
    "vi_dataloader = DataLoader(vi_word2vec_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1]) torch.Size([64, 4])\n",
      "torch.Size([64, 1]) torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "# check noise_vi and vi dataloader\n",
    "for targets, contexts in noise_vi_dataloader:\n",
    "    print(targets.shape, contexts.shape)\n",
    "    break\n",
    "\n",
    "for targets, contexts in vi_dataloader:\n",
    "    print(targets.shape, contexts.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
