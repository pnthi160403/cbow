{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/phamngocthi/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from underthesea import word_tokenize\n",
    "from pyvi import ViTokenizer\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import re\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchmetrics import Recall, Precision, FBetaScore, Accuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"./dataset/data/tokenizer.json\"\n",
    "train_data_path = \"./dataset/data/train.csv\"\n",
    "test_data_path = \"./dataset/data/test.csv\"\n",
    "noise_vi_path = \"./dataset/data/noise_vi.csv\"\n",
    "vi_path = \"./dataset/data/vi.csv\"\n",
    "noise_vi_split_path = \"./dataset/data/noise_vi_split.csv\"\n",
    "vi_split_path = \"./dataset/data/vi_split.csv\"\n",
    "vocab_size = 20000\n",
    "context_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_vi</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>họ có mối quan hệ san xuất chặt chẽ phi thường...</td>\n",
       "      <td>họ có mối quan hệ sản xuất chặt chẽ phi thường...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>họ đặt mình vào máy chụp nao bộ -- có thể cách...</td>\n",
       "      <td>họ đặt mình vào máy chụp não bộ -- có thể cách...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đây là bài mo đầu cho brian ẽno , sự cảm lhậl ...</td>\n",
       "      <td>đây là bài mở đầu cho brian eno , sự cảm nhận ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hàng hoá là nen tảng của nền kinh tế nông nghi...</td>\n",
       "      <td>hàng hoá là nền tảng của nền kinh tế nông nghi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>không một quốc gia mv latinh nào khác có nhiều...</td>\n",
       "      <td>không một quốc gia mỹ latinh nào khác có nhiều...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424692</th>\n",
       "      <td>còn đó chính xác những gĩ mà người mĩ yêu cầu .</td>\n",
       "      <td>còn đó chính xác những gì mà người mỹ yêu cầu .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424693</th>\n",
       "      <td>tôi sệ nói , chên hết chính là nạn buôn lậu ng...</td>\n",
       "      <td>tôi sẽ nói , trên hết chính là nạn buôn lậu ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424694</th>\n",
       "      <td>trên thiên đường bạl thấy những người thuê đôi...</td>\n",
       "      <td>trên thiên đường bạn thấy những người thuê đôi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424695</th>\n",
       "      <td>vâng , moi người đã nêu lên ý kiến của mình tr...</td>\n",
       "      <td>vâng , mỗi người đã nêu lên ý kiến của mình tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424696</th>\n",
       "      <td>và bệnh nhân cơ bản với điều kiền chữa trị trị...</td>\n",
       "      <td>và bệnh nhân cơ bản với điều kiện chữa trị tốt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424697 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 noise_vi  \\\n",
       "0       họ có mối quan hệ san xuất chặt chẽ phi thường...   \n",
       "1       họ đặt mình vào máy chụp nao bộ -- có thể cách...   \n",
       "2       đây là bài mo đầu cho brian ẽno , sự cảm lhậl ...   \n",
       "3       hàng hoá là nen tảng của nền kinh tế nông nghi...   \n",
       "4       không một quốc gia mv latinh nào khác có nhiều...   \n",
       "...                                                   ...   \n",
       "424692    còn đó chính xác những gĩ mà người mĩ yêu cầu .   \n",
       "424693  tôi sệ nói , chên hết chính là nạn buôn lậu ng...   \n",
       "424694  trên thiên đường bạl thấy những người thuê đôi...   \n",
       "424695  vâng , moi người đã nêu lên ý kiến của mình tr...   \n",
       "424696  và bệnh nhân cơ bản với điều kiền chữa trị trị...   \n",
       "\n",
       "                                                       vi  \n",
       "0       họ có mối quan hệ sản xuất chặt chẽ phi thường...  \n",
       "1       họ đặt mình vào máy chụp não bộ -- có thể cách...  \n",
       "2       đây là bài mở đầu cho brian eno , sự cảm nhận ...  \n",
       "3       hàng hoá là nền tảng của nền kinh tế nông nghi...  \n",
       "4       không một quốc gia mỹ latinh nào khác có nhiều...  \n",
       "...                                                   ...  \n",
       "424692    còn đó chính xác những gì mà người mỹ yêu cầu .  \n",
       "424693  tôi sẽ nói , trên hết chính là nạn buôn lậu ng...  \n",
       "424694  trên thiên đường bạn thấy những người thuê đôi...  \n",
       "424695  vâng , mỗi người đã nêu lên ý kiến của mình tr...  \n",
       "424696  và bệnh nhân cơ bản với điều kiện chữa trị tốt...  \n",
       "\n",
       "[424697 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_vi</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cái tật sạo không bỏ</td>\n",
       "      <td>cái tật xạo không bỏ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cái ông lầy nàm thơ</td>\n",
       "      <td>cái ông này làm thơ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sài lu không ăn thua gì</td>\n",
       "      <td>xài lu không ăn thua gì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>người hiểu xẽ nhìn ra</td>\n",
       "      <td>người hiểu sẽ nhìn ra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sử sự không còn gì để nói</td>\n",
       "      <td>xử sự không còn gì để nói</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>anh ơi em ở miền trung . khí hậu nắng nóng . m...</td>\n",
       "      <td>anh ơi em ở miền trung . khí hậu nắng nóng , m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>gạo đang tăng chống mặt luôn</td>\n",
       "      <td>gạo đang tăng chóng mặt luôn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>bị đình chỉ nghề diển viên</td>\n",
       "      <td>bị đình chỉ nghề diễn viên</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>tuân úc theo tào tháo nghĩ tào tháo sẽ phục hư...</td>\n",
       "      <td>tuân úc theo tào tháo nghĩ tào tháo sẽ phục hư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>tại sao mình phải chạy ra khỏi nhà khi mình có...</td>\n",
       "      <td>tại sao mình phải chạy ra khỏi nhà khi mình có...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               noise_vi  \\\n",
       "0                               cái tật sạo không bỏ   \n",
       "1                                   cái ông lầy nàm thơ   \n",
       "2                             sài lu không ăn thua gì   \n",
       "3                                 người hiểu xẽ nhìn ra   \n",
       "4                             sử sự không còn gì để nói   \n",
       "...                                                 ...   \n",
       "3126  anh ơi em ở miền trung . khí hậu nắng nóng . m...   \n",
       "3127                       gạo đang tăng chống mặt luôn   \n",
       "3128                         bị đình chỉ nghề diển viên   \n",
       "3129  tuân úc theo tào tháo nghĩ tào tháo sẽ phục hư...   \n",
       "3130  tại sao mình phải chạy ra khỏi nhà khi mình có...   \n",
       "\n",
       "                                                     vi  \n",
       "0                                cái tật xạo không bỏ  \n",
       "1                                   cái ông này làm thơ  \n",
       "2                              xài lu không ăn thua gì  \n",
       "3                                 người hiểu sẽ nhìn ra  \n",
       "4                             xử sự không còn gì để nói  \n",
       "...                                                 ...  \n",
       "3126  anh ơi em ở miền trung . khí hậu nắng nóng , m...  \n",
       "3127                       gạo đang tăng chóng mặt luôn  \n",
       "3128                         bị đình chỉ nghề diễn viên  \n",
       "3129  tuân úc theo tào tháo nghĩ tào tháo sẽ phục hư...  \n",
       "3130  tại sao mình phải chạy ra khỏi nhà khi mình có...  \n",
       "\n",
       "[3131 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(test_data_path)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split and save noise_vi and vi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split noise_vi and vi column\n",
    "noise_vi_dataset = {\"text\": train_data['noise_vi']}\n",
    "vi_dataset = { \"text\": train_data['vi']}\n",
    "\n",
    "# save noise_vi and vi dataset with 1 column name = text\n",
    "pd.DataFrame(noise_vi_dataset).to_csv(noise_vi_path, index=False)\n",
    "pd.DataFrame(vi_dataset).to_csv(vi_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 424697 examples [00:01, 327957.01 examples/s]\n",
      "Generating train split: 424697 examples [00:01, 356572.17 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['họ có mối quan hệ san xuất chặt chẽ phi thường với nhữg tổ chùc am ninh ở cả hai phía biên với .', 'họ đặt mình vào máy chụp nao bộ -- có thể cách đó không chính xác , nhưg tôi không tâm -- họ được cho cho xem xem một bộ phin kinh dị , sau do miêu tẵ nhữg cảm giác vời vợ của mình .', 'đây là bài mo đầu cho brian ẽno , sự cảm lhậl âm nhạc của tôi rất giống thế này .']\n",
      "['họ có mối quan hệ sản xuất chặt chẽ phi thường với những tổ chức an ninh ở cả hai phía biên giới .', 'họ đặt mình vào máy chụp não bộ -- có thể cách đó không chính xác , nhưng tôi không quan tâm -- họ được cho xem xem một bộ phim kinh dị , sau đó miêu tả những cảm giác với vợ của mình .', 'đây là bài mở đầu cho brian eno , sự cảm nhận âm nhạc của tôi rất giống thế này .']\n"
     ]
    }
   ],
   "source": [
    "# read file noise_vi and vi\n",
    "noise_vi_dataset = load_dataset('csv', data_files=noise_vi_path)\n",
    "vi_dataset = load_dataset('csv', data_files=vi_path)\n",
    "\n",
    "# check noise_vi and vi dataset\n",
    "print(noise_vi_dataset[\"train\"][\"text\"][:3])\n",
    "print(vi_dataset[\"train\"][\"text\"][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle word with context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_word_with_context_size(token_ids, context_size):\n",
    "    targets = []\n",
    "    contexts = []\n",
    "\n",
    "    for i in range(context_size, len(token_ids) - context_size):\n",
    "        target_word = [token_ids[i]]\n",
    "        context_words = [token_ids[j] for j in range(i - context_size, i)] + \\\n",
    "                        [token_ids[j] for j in range(i + 1, i + context_size + 1)]\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context_words)\n",
    "\n",
    "    return targets, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, context_size):\n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.targets = []\n",
    "        self.contexts = []\n",
    "        for text in dataset:\n",
    "            token_ids = self.tokenizer.encode(text).ids\n",
    "            target, context = handle_word_with_context_size(token_ids, self.context_size)\n",
    "            self.targets += target\n",
    "            self.contexts += context\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.targets[idx]), torch.tensor(self.contexts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset split 150000 item and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save noise_vi and vi split\n",
    "pd.DataFrame(noise_vi_dataset[\"train\"].select(range(150000))).to_csv(noise_vi_split_path, index=False)\n",
    "pd.DataFrame(vi_dataset[\"train\"].select(range(150000))).to_csv(vi_split_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 150000 examples [00:00, 347210.21 examples/s]\n",
      "Generating train split: 150000 examples [00:00, 343048.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load noise_vi and vi split\n",
    "noise_vi_split_dataset = load_dataset('csv', data_files=noise_vi_split_path)\n",
    "vi_split_dataset = load_dataset('csv', data_files=vi_split_path)\n",
    "\n",
    "# check noise_vi and vi split dataset\n",
    "print(noise_vi_split_dataset)\n",
    "print(vi_split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sents(dataset):\n",
    "    for item in dataset:\n",
    "        yield item\n",
    "\n",
    "def get_tokenizer(dataset):\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = WordLevelTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\"])\n",
    "    tokenizer.train_from_iterator(get_all_sents(dataset), trainer)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "13148\n"
     ]
    }
   ],
   "source": [
    "tokenizer_noise_vi = get_tokenizer(noise_vi_split_dataset[\"train\"][\"text\"])\n",
    "tokenizer_vi = get_tokenizer(vi_split_dataset[\"train\"][\"text\"])\n",
    "print(tokenizer_noise_vi.get_vocab_size())\n",
    "print(tokenizer_vi.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_vi_word2vec_dataset = Word2VecDataset(noise_vi_split_dataset[\"train\"][\"text\"], tokenizer_noise_vi, context_size)\n",
    "vi_word2vec_dataset = Word2VecDataset(vi_split_dataset[\"train\"][\"text\"], tokenizer_vi, context_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 64\n",
    "noise_vi_dataloader = DataLoader(noise_vi_word2vec_dataset, batch_size=batch_size, shuffle=True)\n",
    "vi_dataloader = DataLoader(vi_word2vec_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1]) torch.Size([64, 4])\n",
      "torch.Size([64, 1]) torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "# check noise_vi and vi dataloader\n",
    "for targets, contexts in noise_vi_dataloader:\n",
    "    print(targets.shape, contexts.shape)\n",
    "    break\n",
    "\n",
    "for targets, contexts in vi_dataloader:\n",
    "    print(targets.shape, contexts.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
